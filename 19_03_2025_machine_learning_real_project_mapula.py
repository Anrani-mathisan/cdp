# -*- coding: utf-8 -*-
"""19/03/2025 machine learning real project mapula

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14VxghpW9x0yC2Vf55lpVTmKSsmU6K0B_

IMPORT REQUIRED LIBRARIES / LOAD DATASET
"""

import numpy as np
import pandas as pd
from scipy import stats
from scipy.stats import zscore
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from imblearn.over_sampling import SMOTE
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

file_path = 'D:\CustomerDepositPrediction\Bank_ml_project_balanced_50_50.csv'
df = pd.read_csv(file_path)
print(df.head())

"""SUMMARY THE DATASET"""

df.info()

"""VIEWING THE DATASET"""

df.head(20)

"""VIEWING COLUMN NAMES"""

df.columns

"""VIEWING DATATYPES OF THE COLUMNS"""

df.dtypes

"""VIEWING NO.OF ROWS AND COLUMNS"""

print(df.shape[0])
print(df.shape[1])

"""FIND UNIQUE VALUES IN THE DATASET"""

df.nunique()

"""IDENTIFYING TARGET VARIABLE AND ITS UNIQUE VALUES"""

df.deposit.unique()

"""DETECTING DUPLICATE VALUES IN THE DATASET"""

df.duplicated().sum()

"""DROPPING DUPLICATE VALUES"""

df.drop_duplicates(inplace=True)
df.duplicated().sum()

"""VIEWING THE TARGET VARAIABLE"""

print(df['deposit'].value_counts())

"""CHECKING THE MISSING VALUE IN DATASET/STATISTICS IN THE DATASET"""

print(df.isnull().sum())
print(df.describe())

"""FINDING THE NO.OF NEGATIVE VALUES IN THE DATASET"""

numerical_columns = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']
c = 0
for col in numerical_columns:
    c += (df[col] < 0).sum()
print(f"Number of negative values: {c}")

"""VISUALIZE THE RELATIONSHIP BETWEEN THE COLUMNS"""

# Set the plot style for better visualization
sns.set(style="whitegrid")

# 1. Correlation Matrix for Numerical Features
# Calculate the correlation between numerical columns
numerical_columns = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']
correlation_matrix = df[numerical_columns].corr()

# Plotting the correlation heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix of Numerical Features")
plt.show()

# 2. Countplot for Categorical Variables vs Target ('deposit')
categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']

# Plotting countplots for each categorical column with respect to target variable 'deposit'
plt.figure(figsize=(15, 10))
for i, feature in enumerate(categorical_columns):
    plt.subplot(len(categorical_columns)//3+1, 3, i+1)
    sns.countplot(x=feature, hue='deposit', data=df)
    plt.title(f'Countplot of {feature} vs Deposit')
plt.tight_layout()
plt.show()

# 3. Boxplot or Violin Plot for Numerical vs Categorical Data
plt.figure(figsize=(15, 10))
for i, feature in enumerate(numerical_columns):
    plt.subplot(len(numerical_columns)//3+1, 3, i+1)
    sns.boxplot(x='deposit', y=feature, data=df)
    plt.title(f'{feature} vs Deposit (Boxplot)')
plt.tight_layout()
plt.show()

# Set the plot style for better readability
sns.set(style="whitegrid")

# Categorical vs Target: Simple Bar Plot
plt.figure(figsize=(10, 6))
sns.countplot(x='job', hue='deposit', data=df)
plt.title('Job Type vs Deposit')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='marital', hue='deposit', data=df)
plt.title('Marital Status vs Deposit')
plt.xticks(rotation=45)
plt.show()

# 1. Histogram for Age Distribution
plt.figure(figsize=(8, 6))
sns.histplot(df['age'], bins=20, kde=True)
plt.title('Distribution of Customer Age')
plt.xlabel('Age')
plt.ylabel('Count')
plt.show()

"""Visualizing the outliers"""

# Identify numerical columns
numerical_columns = df.select_dtypes(include=['number']).columns

# Visualize outliers using boxplots
plt.figure(figsize=(12, 6))
for i, column in enumerate(numerical_columns, 1):
    plt.subplot(1, len(numerical_columns), i)
    sns.boxplot(y=df[column])
    plt.title(f'Boxplot of {column}')
plt.tight_layout()
plt.show()

"""Detecting the outliers using Z_Scores"""

# Calculate z-scores for numerical columns
z_scores = df[numerical_columns].apply(zscore)

# Set a threshold (e.g., 3)
threshold = 3

# Find outliers
outliers_z = {}
for column in numerical_columns:
    outliers_z[column] = df[z_scores[column].abs() > threshold]
    print(f"Outliers in column '{column}': {len(outliers_z[column])}Â rows")

"""Handle ouliers using IQR method"""

numerical_features = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']

def handle_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    data[column] = np.clip(data[column], lower_bound, upper_bound)
    return data

for feature in numerical_features:
    df = handle_outliers_iqr(df, feature)

"""viewing the outliers"""

z_scores = df[numerical_columns].apply(zscore)

# Set a threshold (e.g., 3)
threshold = 3

# Find outliers
outliers_z = {}
for column in numerical_columns:
    outliers_z[column] = df[z_scores[column].abs() > threshold]
    print(f"Outliers in column '{column}': {len(outliers_z[column])}Â rows")

"""CHANGING CATEGORICAL VARIABLE INTO NUMERIC VALUES"""

categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'deposit']

label_encoder = LabelEncoder()

for feature in categorical_features:
    df[feature] = label_encoder.fit_transform(df[feature])

print(df.head())

"""HANDLE DATA IMBALANCE"""

print(df['job'].value_counts())

X = df.drop(columns='deposit')  # Assuming 'Target' is the column you're predicting
y = df['deposit']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE to the training data
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Check the new class distribution
print(pd.Series(y_resampled).value_counts())

"""BUILDING THE LOGISTIC REGRESSION MODEL"""

df_encoded = pd.get_dummies(df, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome'], drop_first=True)

# Encode the target variable
df_encoded['deposit'] = LabelEncoder().fit_transform(df['deposit'])

# Define features and target
X, y = df_encoded.drop('deposit', axis=1), df_encoded['deposit']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(StandardScaler().fit_transform(X), y, test_size=0.2, random_state=42)

# Apply SMOTE to oversample the minority class in the training set
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Train logistic regression model
model = LogisticRegression(max_iter=5000, solver='saga')
model.fit(X_train_resampled, y_train_resampled)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix Heatmap
plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues", xticklabels=['No Deposit', 'Deposit'], yticklabels=['No Deposit', 'Deposit'])
plt.xlabel("Predicted"), plt.ylabel("Actual"), plt.title("Confusion Matrix")
plt.show()

# Encode categorical variables using one-hot encoding
df_encoded = pd.get_dummies(df, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome'], drop_first=True)

# Encode target variable
df_encoded['deposit'] = LabelEncoder().fit_transform(df['deposit'])

# Define features and target
X, y = df_encoded.drop('deposit', axis=1), df_encoded['deposit']

# Standardize features
X_train, X_test, y_train, y_test = train_test_split(StandardScaler().fit_transform(X), y, test_size=0.2, random_state=42)

# Train logistic regression model
model = LogisticRegression(max_iter=5000, solver='saga')
model.fit(X_train, y_train)

# Predictions and evaluation
y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix Heatmap
plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues", xticklabels=['No Deposit', 'Deposit'], yticklabels=['No Deposit', 'Deposit'])
plt.xlabel("Predicted"), plt.ylabel("Actual"), plt.title("Confusion Matrix")
plt.show()

model = LogisticRegression(class_weight='balanced', random_state=42)

# Fit the model
model.fit(X_train, y_train)

# Make predictions and evaluate the model
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

"""BUILDING THE DECISION TREE MODEL"""

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define hyperparameter grid
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Perform Grid Search
grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Train best Decision Tree model
best_model = grid_search.best_estimator_
best_model.fit(X_train, y_train)

# Predictions and evaluation
y_pred = best_model.predict(X_test)
print(f"Best Parameters: {grid_search.best_params_}\n")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix Heatmap
plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues", xticklabels=['No Deposit', 'Deposit'], yticklabels=['No Deposit', 'Deposit'])
plt.xlabel("Predicted"), plt.ylabel("Actual"), plt.title("Confusion Matrix")
plt.show()

"""BUILDING THE RANDOM FOREST MODEL"""

# Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=200, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=42)
rf_model.fit(X_train, y_train)

# Predictions and evaluation
y_pred = rf_model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix Heatmap
plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues", xticklabels=['No Deposit', 'Deposit'], yticklabels=['No Deposit', 'Deposit'])
plt.xlabel("Predicted"), plt.ylabel("Actual"), plt.title("Confusion Matrix")
plt.show()

"""SUPPORT VECTOR MACHINE MODEL"""

# Feature Scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train SVM model
# Assuming 'SVC' is imported from sklearn.svm
from sklearn.svm import SVC
model = SVC(kernel='rbf', random_state=42)
model.fit(X_train, y_train)

# Predictions and evaluation
y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix Heatmap
plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues", xticklabels=['No Deposit', 'Deposit'], yticklabels=['No Deposit', 'Deposit'])
plt.xlabel("Predicted"), plt.ylabel("Actual"), plt.title("Confusion Matrix")
plt.show()

"""KNN MODEL"""

# Feature Scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train KNN model
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)

# Predictions and evaluation
y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix Heatmap
plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues", xticklabels=['No Deposit', 'Deposit'], yticklabels=['No Deposit', 'Deposit'])
plt.xlabel("Predicted"), plt.ylabel("Actual"), plt.title("Confusion Matrix")
plt.show()

# most important features (Duration, Pdays, Previous, Balance, Campaign, Job, and Poutcome.)

# Selecting Important Features
important_features = ["duration", "pdays", "previous", "balance", "campaign", "job", "poutcome", "deposit"]  # 'deposit' is the target column
df = df[important_features]

# Splitting Features & Target
X = df.drop(columns=["deposit"])  # Features
y = df["deposit"]  # Target variable

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest Model
model = RandomForestClassifier(n_estimators=200, criterion='gini', max_depth=None, min_samples_split=2, random_state=42)
model.fit(X_train, y_train)

# Model Evaluation
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.4f}")

def predict_deposit():
    print("\nEnter User Details for Prediction:")
    duration = float(input("Call Duration (in seconds): "))
    pdays = int(input("Days Passed After Previous Campaign (-1 if not contacted before): "))
    previous = int(input("Number of Contacts Before This Campaign: "))
    balance = float(input("Account Balance: "))
    campaign = int(input("Number of Contacts During Campaign: "))
    job = int(input("Job (Encoded as Number): "))
    poutcome = int(input("Previous Campaign Outcome (Encoded as Number): "))

    # Creating Input Array
    user_data = np.array([[duration, pdays, previous, balance, campaign, job, poutcome]])

    # Prediction
    prediction = model.predict(user_data)[0]
    result = "Deposit" if prediction == 1 else "No Deposit"

    print(f"\nPredicted Class for User: {result}")

# Run Prediction Function
predict_deposit()

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
# Separate features and target
X = df.drop(columns=['deposit'])
y = df['deposit']

# Convert categorical variables to numerical using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Split data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Apply SMOTE to balance only the training data
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Check new class distribution
print("Class distribution before SMOTE:\n", y_train.value_counts(normalize=True))
print("Class distribution after SMOTE:\n", y_train_resampled.value_counts(normalize=True))

# Separate features and target
X = df.drop(columns=['deposit'])
y = df['deposit']

# Convert categorical variables to numerical using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Split data into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Train a Random Forest model with class weighting to handle imbalance
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)
y_prob = rf_model.predict_proba(X_test)[:, 1]  # Probability scores for ROC-AUC

# Evaluate model performance
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)

print("Accuracy:", accuracy)
print("ROC-AUC Score:", roc_auc)
print("Classification Report:\n", classification_rep)

# Feature Importance Analysis
feature_importances = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)

# Plot Feature Importances
plt.figure(figsize=(12, 6))
sns.barplot(x=feature_importances[:10], y=feature_importances.index[:10], palette='viridis')
plt.title("Top 10 Important Features")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

# Function to predict deposit for new user input with a custom threshold
def predict_deposit(user_input, threshold=0.3):  # Lower threshold to favor deposits
    user_df = pd.DataFrame([user_input])
    user_df = pd.get_dummies(user_df, drop_first=True)

    # Ensure the input has the same feature columns as the trained model
    missing_cols = set(X.columns) - set(user_df.columns)
    for col in missing_cols:
        user_df[col] = 0

    user_df = user_df[X.columns]  # Reorder columns
    probability = rf_model.predict_proba(user_df)[:, 1][0]

    prediction = 1 if probability >= threshold else 0  # Use custom threshold
    return "Yes, Deposit" if prediction == 1 else "No, No Deposit", probability

# Example user input (replace with actual user input values)
user_input = {
    'age': 35,
    'job_admin.': 0,
    'job_blue-collar': 1,
    'marital_married': 1,
    'education_secondary': 0,
    'balance': 50000,
    'day': 15,
    'duration': 300,
    'campaign': 2,
    'previous': 1,
    'poutcome_success': 1
}

prediction, probability = predict_deposit(user_input)
print(f"Predicted Deposit Status: {prediction} (Probability: {probability:.2f})")

from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE

# Separate features and target
X = df.drop(columns=['deposit'])
y = df['deposit']

# Step 1: Oversample the minority class (Deposit) using SMOTE
smote = SMOTE(sampling_strategy=0.8, random_state=42)  # Increase deposits first
X_smote, y_smote = smote.fit_resample(X, y)

# Step 2: Undersample the majority class (No Deposit) to match 50-50
undersample = RandomUnderSampler(sampling_strategy=1.0, random_state=42)  # Make 50-50
X_balanced, y_balanced = undersample.fit_resample(X_smote, y_smote)

# Save the balanced dataset
balanced_file_path = "Bank_ml_project_balanced_50_50.csv"
df_balanced = pd.DataFrame(X_balanced, columns=X.columns)
df_balanced['deposit'] = y_balanced
df_balanced.to_csv(balanced_file_path, index=False)

# Check new class distribution
print("New class distribution:\n", y_balanced.value_counts(normalize=True) * 100)
print("Balanced dataset saved as:", balanced_file_path)

# Separate features and target
X = df.drop(columns=['deposit'])
y = df['deposit']

# Step 1: Balance the dataset (50% deposit, 50% non-deposit)
desired_ratio = 0.5  # 50% deposit, 50% non-deposit
total_samples = len(y)
target_deposit_count = int(total_samples * desired_ratio)  # 50% deposit
target_non_deposit_count = int(total_samples * (1 - desired_ratio))  # 50% non-deposit

# Apply SMOTE first to increase the minority class
smote = SMOTE(sampling_strategy=0.8, random_state=42)  # Increase deposit first
X_smote, y_smote = smote.fit_resample(X, y)

# Then apply undersampling to match both classes 50-50
undersample = RandomUnderSampler(sampling_strategy=1.0, random_state=42)  # Make 50-50
X_resampled, y_resampled = undersample.fit_resample(X_smote, y_smote)

# Step 2: Train-Test Split (80-20)
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

# Step 3: Train Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight="balanced")
rf_model.fit(X_train, y_train)

# Step 4: Make Predictions
y_pred = rf_model.predict(X_test)

# Step 5: Evaluate Model
print("ðŸ”¹ Model Accuracy:", accuracy_score(y_test, y_pred))
print("ðŸ”¹ Classification Report:\n", classification_report(y_test, y_pred))

# Save the balanced dataset
balanced_file_path = "Bank_ml_project_balanced_50_50.csv"
df_resampled = pd.DataFrame(X_resampled, columns=X.columns)
df_resampled['deposit'] = y_resampled
df_resampled.to_csv(balanced_file_path, index=False)

print("\nâœ… Balanced dataset saved as:", balanced_file_path)

# ðŸ”¹ Function to Predict Deposit for New User Input
def predict_deposit(user_input):
    user_df = pd.DataFrame([user_input])
    user_df = pd.get_dummies(user_df, drop_first=True)

    # Ensure the input has the same feature columns as the trained model
    missing_cols = set(X.columns) - set(user_df.columns)
    for col in missing_cols:
        user_df[col] = 0

    user_df = user_df[X.columns]  # Reorder columns
    prediction = rf_model.predict(user_df)[0]
    probability = rf_model.predict_proba(user_df)[:, 1][0]

    return "Yes, Deposit" if prediction == 1 else "No, No Deposit", probability

# Example User Input for Prediction
user_input = {
    'age': 35,
    'job_admin.': 0,
    'job_blue-collar': 1,
    'marital_married': 1,
    'education_secondary': 0,
    'balance': 50000,
    'day': 15,
    'duration': 300,
    'campaign': 2,
    'previous': 1,
    'poutcome_success': 1
}

# Make Prediction for User Input
prediction, probability = predict_deposit(user_input)
print(f"ðŸ”¹ Predicted Deposit Status: {prediction} (Probability: {probability:.2f})")

# Function to take user input dynamically
def get_user_input():
    print("Enter customer details for deposit prediction:")

    user_input = {
        'age': int(input("Age: ")),
        'job_admin.': int(input("Job Admin (1 for Yes, 0 for No): ")),
        'job_blue-collar': int(input("Job Blue-Collar (1 for Yes, 0 for No): ")),
        'marital_married': int(input("Married (1 for Yes, 0 for No): ")),
        'education_secondary': int(input("Education Secondary (1 for Yes, 0 for No): ")),
        'balance': float(input("Account Balance: ")),
        'day': int(input("Day of the Month: ")),
        'duration': int(input("Call Duration (in seconds): ")),
        'campaign': int(input("Number of Contacts During Campaign: ")),
        'previous': int(input("Number of Contacts Before This Campaign: ")),
        'poutcome_success': int(input("Previous Campaign Successful? (1 for Yes, 0 for No): "))
    }

    return user_input

# Get user input
user_input = get_user_input()

# Make Prediction for User Input
prediction, probability = predict_deposit(user_input)
print(f"\nðŸ”¹ Predicted Deposit Status: {prediction} (Probability: {probability:.2f})")